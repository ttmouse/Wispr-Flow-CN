from funasr import AutoModel
import numpy as np
import os
import sys
import logging
import re
from contextlib import redirect_stdout, redirect_stderr
import io
import time
from concurrent.futures import ThreadPoolExecutor
import math

# è®¾ç½® modelscope æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œå‡å°‘ä¸å¿…è¦çš„ä¿¡æ¯
logging.getLogger('modelscope').setLevel(logging.WARNING)

class FunASREngine:
    def __init__(self, settings_manager=None):
        try:
            # ä¿å­˜è®¾ç½®ç®¡ç†å™¨å¼•ç”¨
            self.settings_manager = settings_manager
            # åˆå§‹åŒ–çŠ¶æ€æ ‡å¿—
            self.is_ready = False
            # è·å–åº”ç”¨ç¨‹åºçš„åŸºç¡€è·¯å¾„
            if getattr(sys, 'frozen', False):
                application_path = sys._MEIPASS
            else:
                application_path = os.path.dirname(os.path.abspath(__file__))
            
            # è®¾ç½® MODELSCOPE_CACHE ç¯å¢ƒå˜é‡
            cache_dir = os.path.join(application_path, 'modelscope', 'hub')
            os.environ['MODELSCOPE_CACHE'] = cache_dir
            
            # åˆå§‹åŒ–çƒ­è¯åˆ—è¡¨
            self.hotwords = []
            hotwords_file = os.path.join(os.path.dirname(application_path), "resources", "hotwords.txt")
            if os.path.exists(hotwords_file):
                with open(hotwords_file, 'r', encoding='utf-8') as f:
                    self.hotwords = [line.strip() for line in f 
                                   if line.strip() and not line.strip().startswith('#')]
                print(f"âœ… çƒ­è¯åŠ è½½æˆåŠŸ: å…±åŠ è½½ {len(self.hotwords)} ä¸ªçƒ­è¯")
                print(f"ğŸ“ çƒ­è¯åˆ—è¡¨: {', '.join(self.hotwords[:10])}{'...' if len(self.hotwords) > 10 else ''}")
            else:
                print(f"âš ï¸ çƒ­è¯æ–‡ä»¶ä¸å­˜åœ¨: {hotwords_file}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            asr_model_dir = os.path.join(cache_dir, 'damo', 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch')
            punc_model_dir = os.path.join(cache_dir, 'damo', 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch')
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            
            # ASRæ¨¡å‹æ˜¯å¿…éœ€çš„
            if not os.path.exists(asr_model_dir):
                raise Exception(f"ASRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {asr_model_dir}")
            
            # æ ‡ç‚¹æ¨¡å‹æ˜¯å¯é€‰çš„
            self.has_punc_model = os.path.exists(punc_model_dir)
            if not self.has_punc_model:
                print("âš ï¸ æ ‡ç‚¹æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡æ ‡ç‚¹å¤„ç†")
            
            # ä½¿ç”¨ redirect_stdout æ¥æ•è·è¾“å‡º
            f = io.StringIO()
            with redirect_stdout(f), redirect_stderr(f):
                # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«æ¨¡å‹
                self.model = AutoModel(
                    model=asr_model_dir,
                    model_revision="v2.0.4",
                    disable_update=True
                )
                
                # åªåœ¨æ ‡ç‚¹æ¨¡å‹å­˜åœ¨æ—¶æ‰åŠ è½½
                if self.has_punc_model:
                    self.punc_model = AutoModel(
                        model=punc_model_dir,
                        model_revision="v2.0.4",
                        disable_update=True
                    )
                else:
                    self.punc_model = None
                
            # è®¾ç½®å¼•æ“å°±ç»ªçŠ¶æ€
            self.is_ready = True
            pass  # FunASRå¼•æ“åˆå§‹åŒ–å®Œæˆ
                
        except Exception as e:
            self.is_ready = False
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}\n"
            error_msg += f"ç³»ç»Ÿè·¯å¾„: {sys.path}\n"
            error_msg += f"å½“å‰ç›®å½•: {os.getcwd()}\n"
            error_msg += f"ç¯å¢ƒå˜é‡: MODELSCOPE_CACHE={os.environ.get('MODELSCOPE_CACHE', 'æœªè®¾ç½®')}\n"
            print(error_msg)
            raise

    def preprocess_audio(self, audio_data):
        """éŸ³é¢‘é¢„å¤„ç†
        1. éŸ³é¢‘å½’ä¸€åŒ–
        2. é¢„åŠ é‡ï¼ˆæå‡é«˜é¢‘éƒ¨åˆ†ï¼‰
        3. é™éŸ³æ£€æµ‹å’Œå»é™¤
        """
        try:
            start_time = time.time()
            original_length = len(audio_data)

            # 1. å¿«é€Ÿæ£€æŸ¥æ˜¯å¦éœ€è¦é¢„å¤„ç†
            if len(audio_data) < 1600:  # å°äº100msçš„éŸ³é¢‘ç›´æ¥è·³è¿‡
                return audio_data

            # 2. éŸ³é¢‘å½’ä¸€åŒ–
            try:
                audio_max = float(np.max(np.abs(audio_data)))
                # ä½¿ç”¨Pythonæ ‡é‡è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…NumPyæ•°ç»„æ¯”è¾ƒé”™è¯¯
                if float(audio_max) > 0.0:
                    audio_data = audio_data / audio_max
            except Exception as e:
                print(f"éŸ³é¢‘å½’ä¸€åŒ–å¤„ç†æ—¶å‡ºé”™: {e}")
                # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè·³è¿‡å½’ä¸€åŒ–æ­¥éª¤
                pass

            # 3. æ¡ä»¶æ€§é¢„åŠ é‡ - åªåœ¨é«˜é¢‘ä¿¡å·è¾ƒå¼±æ—¶è¿›è¡Œ
            try:
                diff_mean = float(np.mean(np.abs(np.diff(audio_data))))
                # ä½¿ç”¨Pythonæ ‡é‡è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…NumPyæ•°ç»„æ¯”è¾ƒé”™è¯¯
                if float(diff_mean) < 0.04:
                    preemphasis_coef = 0.97
                    audio_data = np.append(audio_data[0], audio_data[1:] - preemphasis_coef * audio_data[:-1])
            except Exception as e:
                print(f"é¢„åŠ é‡å¤„ç†æ—¶å‡ºé”™: {e}")
                # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè·³è¿‡é¢„åŠ é‡æ­¥éª¤
                pass

            # 4. é™éŸ³æ£€æµ‹å’Œå»é™¤
            chunk_size = 3200  # 200ms at 16kHz
            chunks = np.array_split(audio_data, max(1, len(audio_data) // chunk_size))
            
            # è®¡ç®—èƒ½é‡
            energies = np.array([np.mean(np.abs(chunk)) for chunk in chunks])
            
            # è‡ªé€‚åº”é˜ˆå€¼
            threshold = float(np.mean(energies)) * 0.1
            # ç¡®ä¿ä½¿ç”¨æ ‡é‡å€¼è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…NumPyæ•°ç»„æ¯”è¾ƒé”™è¯¯
            try:
                # ä½¿ç”¨np.greater_equalé¿å…ç›´æ¥æ•°ç»„æ¯”è¾ƒ
                non_silent_mask = np.greater_equal(energies, threshold)
                
                # å¦‚æœé™éŸ³æ¯”ä¾‹è¿‡é«˜æ‰è¿›è¡Œå»é™¤
                # ä½¿ç”¨.mean()å’Œfloat()ç¡®ä¿æ ‡é‡æ¯”è¾ƒ
                silent_ratio = float(np.mean(non_silent_mask.astype(np.float32)))
                # ä½¿ç”¨Pythonæ ‡é‡è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…NumPyæ•°ç»„æ¯”è¾ƒé”™è¯¯
                if silent_ratio < 0.8:
                    # ç¡®ä¿å¸ƒå°”æ•°ç»„ç´¢å¼•è½¬æ¢ä¸ºPythonå¸ƒå°”å€¼
                    non_silent_indices = np.where(non_silent_mask)[0]
                    non_silent_chunks = [chunks[i] for i in non_silent_indices]
                    if len(non_silent_chunks) > 0:  # ä½¿ç”¨len()è€Œä¸æ˜¯ç›´æ¥åˆ¤æ–­åˆ—è¡¨
                        audio_data = np.concatenate(non_silent_chunks)
            except Exception as e:
                print(f"é™éŸ³æ£€æµ‹å¤„ç†æ—¶å‡ºé”™: {e}")
                # å¦‚æœå¤„ç†å¤±è´¥ï¼Œä¿æŒåŸå§‹éŸ³é¢‘æ•°æ®
                pass

            # è®°å½•å¤„ç†ç»“æœ
            process_time = (time.time() - start_time) * 1000
            compression_ratio = len(audio_data) / original_length * 100
            
            return audio_data
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
            return audio_data  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹éŸ³é¢‘

    def _transcribe_single(self, audio_chunk):
        """å¤„ç†å•ä¸ªéŸ³é¢‘å—"""
        try:
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                result = self.model.generate(
                    input=audio_chunk,
                    batch_size_s=200,          # æ¢å¤åˆ°åŸæ¥çš„å€¼
                    use_itn=True,
                    mode='offline',
                    decode_method='greedy_search',  # æ”¹å›greedy_search
                    disable_progress_bar=True,
                    hotwords=[(word, self._get_hotword_weight()) for word in self.hotwords] if self.hotwords else None,  # çƒ­è¯æƒé‡
                    cache_size=2000,           # æ¢å¤åŸæ¥çš„ç¼“å­˜å¤§å°
                    beam_size=5                # æ¢å¤åŸæ¥çš„beam size
                )
                
                # æ·»åŠ çƒ­è¯ä½¿ç”¨æ—¥å¿—
                if self.hotwords:
                    print(f"ğŸ¯ å•å—å¤„ç†ä½¿ç”¨çƒ­è¯: {len(self.hotwords)} ä¸ªçƒ­è¯ï¼Œæƒé‡: {self._get_hotword_weight()}")
            
            if isinstance(result, list) and len(result) > 0:
                text = result[0].get('text', '')
                return text
            return ''
        except Exception as e:
            print(f"âŒ å•å—å¤„ç†å¤±è´¥: {e}")
            return ''

    def _merge_results(self, chunk_results):
        """æ™ºèƒ½åˆå¹¶åˆ†å—ç»“æœ"""
        # ç§»é™¤ç©ºç»“æœ
        texts = [text for text in chunk_results if text.strip()]
        if not texts:
            return ""
        
        # ç®€å•çš„é‡å¤æ£€æµ‹å’Œç§»é™¤
        merged = texts[0]
        for i in range(1, len(texts)):
            current = texts[i]
            # æ£€æŸ¥é‡å éƒ¨åˆ†
            overlap_size = min(len(merged), len(current))
            for size in range(min(overlap_size, 10), 0, -1):  # æœ€å¤šæ£€æŸ¥10ä¸ªå­—çš„é‡å  
                if merged[-size:] == current[:size]:
                    current = current[size:]
                    break
            merged += current
        
        return merged

    def _add_punctuation(self, text):
        """æ·»åŠ æ ‡ç‚¹ç¬¦å·"""
        try:
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                result = self.punc_model.generate(
                    input=text,
                    disable_progress_bar=True,
                    batch_size=1,
                    mode='offline',
                    cache_size=1000,
                    hotword_score=2.0,
                    min_sentence_length=2,
                    hotwords=[(word, self._get_hotword_weight()) for word in self.hotwords] if self.hotwords else None  # çƒ­è¯æƒé‡
                )
                
                # æ·»åŠ çƒ­è¯ä½¿ç”¨æ—¥å¿—
                if self.hotwords:
                    print(f"ğŸ¯ æ ‡ç‚¹æ¨¡å‹ä½¿ç”¨çƒ­è¯: {len(self.hotwords)} ä¸ªçƒ­è¯ï¼Œæƒé‡: {self._get_hotword_weight()}")
            
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('text', text)
            return text
        except Exception as e:
            print(f"âŒ æ ‡ç‚¹å¤„ç†å¤±è´¥: {e}")
            return text

    def transcribe(self, audio_data):
        """è½¬å†™éŸ³é¢‘æ•°æ®"""
        try:
            # å®‰å…¨åœ°æ£€æŸ¥æ•°æ®ç±»å‹ï¼Œé¿å…NumPyæ•°ç»„æ¯”è¾ƒé”™è¯¯
            if hasattr(audio_data, 'dtype') and audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)
            elif not hasattr(audio_data, 'dtype'):
                # å¦‚æœä¸æ˜¯NumPyæ•°ç»„ï¼Œè½¬æ¢ä¸ºNumPyæ•°ç»„
                audio_data = np.array(audio_data, dtype=np.float32)
            
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                # 1. è¯­éŸ³è¯†åˆ«
                result = self.model.generate(
                    input=audio_data,
                    batch_size_s=100,     # å‡å°æ‰¹å¤„ç†å¤§å°
                    use_itn=True,         # å¯ç”¨é€†æ–‡æœ¬æ­£åˆ™åŒ–
                    mode='offline',      # ä½¿ç”¨ç¦»çº¿æ¨¡å¼ä»¥æé«˜å‡†ç¡®ç‡
                    decode_method='greedy_search',  # ä½¿ç”¨è´ªå©ªæœç´¢è§£ç 
                    disable_progress_bar=True,  # ç¦ç”¨è¿›åº¦æ¡
                    hotwords=[(word, self._get_hotword_weight()) for word in self.hotwords] if self.hotwords else None  # çƒ­è¯æƒé‡
                )
                
                # æ·»åŠ çƒ­è¯ä½¿ç”¨æ—¥å¿—
                if self.hotwords:
                    print(f"ğŸ¯ ä¸»è½¬å†™ä½¿ç”¨çƒ­è¯: {len(self.hotwords)} ä¸ªçƒ­è¯ï¼Œæƒé‡: {self._get_hotword_weight()}")
            
            if isinstance(result, list) and len(result) > 0:
                text = result[0].get('text', '')
                
            # 2. æ·»åŠ æ ‡ç‚¹
            text = self._add_punctuation(text)
            
            # 3. å¤„ç†è‹±æ–‡å•è¯é—´çš„ç©ºæ ¼
            processed_text = self._process_text(text)
            
            # 4. å‘éŸ³ç›¸ä¼¼è¯çº é”™
            corrected_text = self._correct_similar_pronunciation(processed_text)
            
            # 5. ä¸å†åœ¨å¼•æ“å±‚æ·»åŠ HTMLæ ‡ç­¾ï¼Œä¿æŒçº¯æ–‡æœ¬
            final_text = corrected_text
            
            return [{"text": final_text}]
            
        except Exception as e:
            print(f"âŒ è½¬å†™å¤±è´¥: {e}")
            raise

    def _process_text(self, text):
        """å¤„ç†æ–‡æœ¬ï¼Œæ·»åŠ è‹±æ–‡å•è¯é—´çš„ç©ºæ ¼"""
        # 1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†ç¦»è‹±æ–‡å•è¯
        def split_english_words(text):
            # åŒ¹é…è¿ç»­çš„è‹±æ–‡å­—æ¯
            words = re.finditer(r'[a-zA-Z]+', text)
            result = []
            last_end = 0
            
            for match in words:
                start, end = match.span()
                # æ·»åŠ éè‹±æ–‡éƒ¨åˆ†
                if start > last_end:
                    result.append(text[last_end:start])
                # æ·»åŠ è‹±æ–‡å•è¯
                word = match.group()
                # ä½¿ç”¨ç®€å•çš„è§„åˆ™åˆ†å‰²è‹±æ–‡å•è¯
                # æ¯”å¦‚ "whatareyou" -> "what are you"
                sub_words = []
                current_word = ""
                for i, char in enumerate(word.lower()):
                    current_word += char
                    # å¸¸è§çš„è‹±æ–‡å•è¯
                    common_words = {'what', 'are', 'you', 'doing', 'how', 'is', 'the', 'this', 'that', 'have', 'has', 'had', 'will', 'would', 'can', 'could', 'should', 'must', 'may', 'might', 'shall'}
                    # å¦‚æœå½“å‰ç§¯ç´¯çš„å­—æ¯æ„æˆäº†ä¸€ä¸ªå¸¸è§å•è¯ï¼Œå¹¶ä¸”å‰©ä½™çš„å­—æ¯ä¹Ÿå¯èƒ½æ„æˆå•è¯
                    if current_word in common_words and i < len(word) - 1:
                        sub_words.append(current_word)
                        current_word = ""
                if current_word:
                    sub_words.append(current_word)
                result.append(' '.join(sub_words))
                last_end = end
            
            # æ·»åŠ å‰©ä½™çš„æ–‡æœ¬
            if last_end < len(text):
                result.append(text[last_end:])
            
            return ''.join(result)
        
        # 2. å¤„ç†æ–‡æœ¬
        processed_text = split_english_words(text)
        
        return processed_text

    def get_model_path(self):
        return "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹"

    def reload_hotwords(self):
        """é‡æ–°åŠ è½½çƒ­è¯"""
        try:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œä¸åˆå§‹åŒ–æ—¶ä¿æŒä¸€è‡´
            if getattr(sys, 'frozen', False):
                application_path = sys._MEIPASS
            else:
                application_path = os.path.dirname(os.path.abspath(__file__))
            
            hotwords_file = os.path.join(os.path.dirname(application_path), "resources", "hotwords.txt")
            if os.path.exists(hotwords_file):
                with open(hotwords_file, "r", encoding="utf-8") as f:
                    self.hotwords = [line.strip() for line in f if line.strip() and not line.startswith("#")]
                print(f"ğŸ”„ çƒ­è¯é‡æ–°åŠ è½½æˆåŠŸ: å…±åŠ è½½ {len(self.hotwords)} ä¸ªçƒ­è¯")
                print(f"ğŸ“ çƒ­è¯åˆ—è¡¨: {', '.join(self.hotwords[:10])}{'...' if len(self.hotwords) > 10 else ''}")
            else:
                print(f"âš ï¸ çƒ­è¯æ–‡ä»¶ä¸å­˜åœ¨: {hotwords_file}")
                self.hotwords = []
        except Exception as e:
            print(f"âŒ é‡æ–°åŠ è½½çƒ­è¯å¤±è´¥: {e}")
            self.hotwords = []
    
    def _get_hotword_weight(self):
        """è·å–çƒ­è¯æƒé‡"""
        if self.settings_manager:
            return self.settings_manager.get_hotword_weight()
        return 80.0  # é»˜è®¤æƒé‡
    
    def _is_pronunciation_correction_enabled(self):
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨å‘éŸ³çº é”™"""
        if self.settings_manager:
            return self.settings_manager.get_pronunciation_correction_enabled()
        return True  # é»˜è®¤å¯ç”¨

    def _correct_similar_pronunciation(self, text):
        """çº æ­£å‘éŸ³ç›¸ä¼¼çš„è¯"""
        if not text or not self._is_pronunciation_correction_enabled():
            return text
        
        # å®šä¹‰å‘éŸ³ç›¸ä¼¼è¯æ˜ å°„è¡¨
        pronunciation_map = {
            'æµ®æ²‰': 'æµ®å±‚',
            'æµ®å°˜': 'æµ®å±‚',
            'æµ®åŸ': 'æµ®å±‚',
            'èƒ¡æˆ': 'æµ®å±‚',  # æ ¹æ®ç”¨æˆ·éœ€æ±‚æ·»åŠ 
            'å«é«˜': 'è¡Œé«˜',  # æ·»åŠ å«é«˜åˆ°è¡Œé«˜çš„æ˜ å°„
            'éŸ©é«˜': 'è¡Œé«˜',  # å¯èƒ½çš„å…¶ä»–å‘éŸ³å˜ä½“
            'æ±‰é«˜': 'è¡Œé«˜',  # å¯èƒ½çš„å…¶ä»–å‘éŸ³å˜ä½“
            'æ¢µé«˜': 'è¡Œé«˜',  # ä¿®å¤æ¢µé«˜è¢«é”™è¯¯è¯†åˆ«ä¸ºè¡Œé«˜çš„é—®é¢˜
        }
        
        corrected_text = text
        
        # åªæœ‰å½“ç›®æ ‡çƒ­è¯åœ¨çƒ­è¯åˆ—è¡¨ä¸­æ—¶æ‰è¿›è¡Œçº é”™
        for similar_word, correct_word in pronunciation_map.items():
            if similar_word in corrected_text and correct_word in self.hotwords:
                corrected_text = corrected_text.replace(similar_word, correct_word)
                print(f"ğŸ”§ å‘éŸ³çº é”™: '{similar_word}' -> '{correct_word}'")
        
        return corrected_text
    
    def _highlight_hotwords(self, text):
        """ä¸å†åœ¨å¼•æ“å±‚æ·»åŠ HTMLæ ‡ç­¾ï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬"""
        # å¼•æ“å±‚ä¸å†å¤„ç†HTMLæ ‡ç­¾ï¼Œä¿æŒçº¯æ–‡æœ¬
        return text
    
    def _post_process_text(self, text):
        """æ–‡æœ¬åå¤„ç†"""
        # 1. ä¿®å¤å¸¸è§çš„é”™è¯¯æ¨¡å¼
        fixes = {
            "æœ‰å¯èƒ½æ˜¯ä¹Ÿæœ‰å¯èƒ½": "æœ‰å¯èƒ½",
            "çš„çš„": "çš„",
            "äº†äº†": "äº†",
            "å—å—": "å—",
            "å•Šå•Š": "å•Š",
            "å—¯å—¯": "å—¯",
            "é—®é¢˜çš„é—®é¢˜": "é—®é¢˜",
        }
        
        for wrong, correct in fixes.items():
            text = text.replace(wrong, correct)
        
        # 2. ä¿®å¤é‡å¤çš„æ ‡ç‚¹
        text = re.sub(r'([ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼šã€])\1+', r'\1', text)
        
        # 3. ä¿®å¤ä¸åˆç†çš„è¯ç»„æ­é…
        text = re.sub(r'è§£å†³äº†(\w+)é—®é¢˜çš„é—®é¢˜', r'è§£å†³äº†\1é—®é¢˜', text)
        
        return text

    def get_model_paths(self):
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„"""
        cache_dir = os.environ.get('MODELSCOPE_CACHE', '')
        asr_model_dir = os.path.join(cache_dir, 'damo', 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch')
        punc_model_dir = os.path.join(cache_dir, 'damo', 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch')
        
        return {
            'asr_model_path': asr_model_dir if os.path.exists(asr_model_dir) else 'æœªæ‰¾åˆ°ASRæ¨¡å‹',
            'punc_model_path': punc_model_dir if os.path.exists(punc_model_dir) else 'æœªæ‰¾åˆ°æ ‡ç‚¹æ¨¡å‹'
        }
    
    def cleanup(self):
        """æ¸…ç†FunASRå¼•æ“èµ„æº"""
        try:
            # é‡ç½®å°±ç»ªçŠ¶æ€
            self.is_ready = False
            
            # æ¸…ç†æ¨¡å‹èµ„æº
            if hasattr(self, 'model') and self.model:
                # å°è¯•é‡Šæ”¾æ¨¡å‹èµ„æº
                del self.model
                self.model = None
                
            if hasattr(self, 'punc_model') and self.punc_model:
                # å°è¯•é‡Šæ”¾æ ‡ç‚¹æ¨¡å‹èµ„æº
                del self.punc_model
                self.punc_model = None
                
            # æ¸…ç†çƒ­è¯åˆ—è¡¨
            if hasattr(self, 'hotwords'):
                self.hotwords.clear()
                
            pass
            
        except Exception as e:
            print(f"âŒ æ¸…ç†FunASRå¼•æ“èµ„æºå¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«é‡Šæ”¾"""
        self.cleanup()